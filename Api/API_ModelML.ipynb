{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y_fkUSPnfKuS",
    "outputId": "881cb4be-a12f-4131-8508-7575378bbbd8",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install uvicorn\n",
    "# !pip install fastapi\n",
    "# !pip install python-multipart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "lljWn3mWao-R"
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import uvicorn\n",
    "import numpy as np\n",
    "import nest_asyncio\n",
    "from enum import Enum\n",
    "from fastapi import FastAPI, UploadFile, File, HTTPException\n",
    "from fastapi.responses import StreamingResponse\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import joblib\n",
    "import librosa\n",
    "\n",
    "def extract(file):\n",
    "    source, sr = librosa.load(file, res_type=\"kaiser_fast\")\n",
    "    stft = np.abs(librosa.stft(source))\n",
    "\n",
    "    mfcc = librosa.feature.mfcc(y=source, sr=sr, n_mfcc=13)\n",
    "    chroma = librosa.feature.chroma_stft(S=stft, sr=sr)\n",
    "    mel = librosa.feature.melspectrogram(y=source, sr=sr)\n",
    "    zrc = librosa.feature.zero_crossing_rate(source)\n",
    "\n",
    "    mfcc = np.mean(mfcc, axis=1)\n",
    "    chroma = np.mean(chroma, axis=1)\n",
    "    mel = np.mean(mel, axis=1)\n",
    "    zcr = np.mean(zrc)\n",
    "\n",
    "    return mfcc, chroma, mel, zcr\n",
    "\n",
    "def scale(features, name):\n",
    "    scaler = joblib.load(f\"weights/scalers/scaler_{name}.save\")\n",
    "\n",
    "    return scaler.transform(features.reshape(1, -1))\n",
    "\n",
    "def predict(file):\n",
    "\n",
    "    mfcc, chroma, mel, zcr = extract(file)\n",
    "    \n",
    "    zcr = scale(zcr, \"zcr\").reshape(-1)\n",
    "    mfcc = scale(mfcc, \"mfcc\").reshape(-1)\n",
    "    chroma = scale(chroma, \"chroma\").reshape(-1)\n",
    "    mel = scale(mel, \"mel\").reshape(-1)\n",
    "\n",
    "    x = np.concatenate([zcr, mfcc, chroma, mel], axis=0).reshape(1, -1)\n",
    "\n",
    "    res = 0\n",
    "\n",
    "    model_list = os.listdir(f\"weights/models/\")\n",
    "\n",
    "    for name in model_list:\n",
    "        model = pickle.load(open(f\"weights/models/\" + name, encoding=\"utf8\"))\n",
    "\n",
    "        res += model.predict(x)\n",
    "\n",
    "    return (res /len(model_list))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v2kYKYbpe368",
    "outputId": "a1fb73ea-7a6f-47fd-940c-310218947cfd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [4680]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:51609 - \"GET /docs HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:51609 - \"GET /openapi.json HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:58564 - \"POST /predict HTTP/1.1\" 500 Internal Server Error\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:    Exception in ASGI application\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\anaconda3\\envs\\AICOVIDVN__KO-Hackathon-II-2021\\lib\\site-packages\\uvicorn\\protocols\\http\\h11_impl.py\", line 373, in run_asgi\n",
      "    result = await app(self.scope, self.receive, self.send)\n",
      "  File \"D:\\anaconda3\\envs\\AICOVIDVN__KO-Hackathon-II-2021\\lib\\site-packages\\uvicorn\\middleware\\proxy_headers.py\", line 75, in __call__\n",
      "    return await self.app(scope, receive, send)\n",
      "  File \"D:\\anaconda3\\envs\\AICOVIDVN__KO-Hackathon-II-2021\\lib\\site-packages\\fastapi\\applications.py\", line 208, in __call__\n",
      "    await super().__call__(scope, receive, send)\n",
      "  File \"D:\\anaconda3\\envs\\AICOVIDVN__KO-Hackathon-II-2021\\lib\\site-packages\\starlette\\applications.py\", line 112, in __call__\n",
      "    await self.middleware_stack(scope, receive, send)\n",
      "  File \"D:\\anaconda3\\envs\\AICOVIDVN__KO-Hackathon-II-2021\\lib\\site-packages\\starlette\\middleware\\errors.py\", line 181, in __call__\n",
      "    raise exc from None\n",
      "  File \"D:\\anaconda3\\envs\\AICOVIDVN__KO-Hackathon-II-2021\\lib\\site-packages\\starlette\\middleware\\errors.py\", line 159, in __call__\n",
      "    await self.app(scope, receive, _send)\n",
      "  File \"D:\\anaconda3\\envs\\AICOVIDVN__KO-Hackathon-II-2021\\lib\\site-packages\\starlette\\exceptions.py\", line 82, in __call__\n",
      "    raise exc from None\n",
      "  File \"D:\\anaconda3\\envs\\AICOVIDVN__KO-Hackathon-II-2021\\lib\\site-packages\\starlette\\exceptions.py\", line 71, in __call__\n",
      "    await self.app(scope, receive, sender)\n",
      "  File \"D:\\anaconda3\\envs\\AICOVIDVN__KO-Hackathon-II-2021\\lib\\site-packages\\starlette\\routing.py\", line 580, in __call__\n",
      "    await route.handle(scope, receive, send)\n",
      "  File \"D:\\anaconda3\\envs\\AICOVIDVN__KO-Hackathon-II-2021\\lib\\site-packages\\starlette\\routing.py\", line 241, in handle\n",
      "    await self.app(scope, receive, send)\n",
      "  File \"D:\\anaconda3\\envs\\AICOVIDVN__KO-Hackathon-II-2021\\lib\\site-packages\\starlette\\routing.py\", line 52, in app\n",
      "    response = await func(request)\n",
      "  File \"D:\\anaconda3\\envs\\AICOVIDVN__KO-Hackathon-II-2021\\lib\\site-packages\\fastapi\\routing.py\", line 226, in app\n",
      "    raw_response = await run_endpoint_function(\n",
      "  File \"D:\\anaconda3\\envs\\AICOVIDVN__KO-Hackathon-II-2021\\lib\\site-packages\\fastapi\\routing.py\", line 159, in run_endpoint_function\n",
      "    return await dependant.call(**values)\n",
      "  File \"C:\\Users\\GL75\\AppData\\Local\\Temp/ipykernel_4680/617458580.py\", line 18, in prediction\n",
      "    pred = predict(fileUpload.file)\n",
      "  File \"C:\\Users\\GL75\\AppData\\Local\\Temp/ipykernel_4680/2731500502.py\", line 42, in predict\n",
      "    model = pickle.load(open(f\"weights/models/\" + name, encoding=\"utf8\"))\n",
      "  File \"D:\\anaconda3\\envs\\AICOVIDVN__KO-Hackathon-II-2021\\lib\\codecs.py\", line 322, in decode\n",
      "    (result, consumed) = self._buffer_decode(data, self.errors, final)\n",
      "UnicodeDecodeError: 'utf-8' codec can't decode byte 0x80 in position 0: invalid start byte\n"
     ]
    }
   ],
   "source": [
    "app = FastAPI(title='Deploying with FastAPI')\n",
    "\n",
    "\n",
    "# By using @app.get(\"/\") you are allowing the GET method to work for the / endpoint.\n",
    "\n",
    "@app.get(\"/\")\n",
    "def home():\n",
    "    return \"Congratulations! Your API is working as expected. Author: Tung Khong Manh. Now head over to \" \\\n",
    "           \"http://localhost:8000/docs. \"\n",
    "\n",
    "\n",
    "@app.post(\"/predict\")\n",
    "async def prediction(fileUpload: UploadFile = File(...)):\n",
    "    filename = fileUpload.filename\n",
    "    fileExtension = filename.split(\".\")[-1] in (\"wav\")\n",
    "    if not fileExtension:\n",
    "        raise HTTPException(status_code=415, detail=\"Unsupported file provided.\")\n",
    "    pred = predict(fileUpload.file)\n",
    "    return {\"result\": 0.7}\n",
    "   \n",
    "# Allows the server to be run in this interactive environment\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Host depends on the setup you selected (docker or virtual env)\n",
    "host = \"0.0.0.0\" if os.getenv(\"DOCKER-SETUP\") else \"127.0.0.1\"\n",
    "\n",
    "# Spin up the server!\n",
    "uvicorn.run(app, host=host, port=8000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AwcNC_Hgfau3"
   },
   "outputs": [],
   "source": [
    "# Allows the server to be run in this interactive environment\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Host depends on the setup you selected (docker or virtual env)\n",
    "host = \"0.0.0.0\" if os.getenv(\"DOCKER-SETUP\") else \"127.0.0.1\"\n",
    "\n",
    "# Spin up the server!    \n",
    "uvicorn.run(app, host=host, port=8000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "API_ModelML.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
